import os
import re
import time
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes, CallbackQueryHandler
import google.generativeai as genai
from pdf2image import convert_from_path
from PIL import Image
import logging

# Configure Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Configuration for Google Gemini API
class Config:
    def __init__(self):
        self.api_key = None
        self.model_name = "models/gemini-2.0-pro-exp"  # Default model
        self.chunk_size = 5000
        self.max_tokens = 8000
        self.model = None
        self.fixed_ocr_model = "models/gemini-2.0-flash-lite"
        self.mode = "mcq"  # Default mode is MCQ
        self.simple_prompt = (
            "From the following text, create a set of simple quiz questions and answers in Hindi with explanations:\n"
            "- Focus on key facts like names, years, and terms.\n"
            "- Format each pair as: **Q:** [Question] | **A:** [Answer] | **E:** [Explanation]\n"
            "- Keep it concise and in Hindi.\n"
            "Text:\n{}"
        )
        self.mcq_prompt = (
            "From the following text, create exam-oriented multiple choice questions (MCQs) in Hindi with explanations:\n"
            "- Cover the entire content comprehensively without adding or removing information.\n"
            "- Focus on key facts like names, years, and terms to make it exam-ready.\n"
            "- Format each MCQ as:\n"
            "  Q: [Question]\n"
            "  A: [Option A]\n"
            "  B: [Option B]\n"
            "  C: [Option C]\n"
            "  D: [Option D]\n"
            "  Correct: [A/B/C/D]\n"
            "  E: [Explanation]\n"
            "- Keep it concise and in Hindi.\n"
            "Text:\n{}"
        )

    def set_api_key(self, api_key):
        self.api_key = api_key
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(self.model_name)

    def set_model(self, model_name):
        self.model_name = model_name
        if self.api_key:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel(self.model_name)

    def set_mode(self, mode):
        self.mode = mode.lower()

    def is_pro_model(self):
        pro_models = [
            "gemini-1.0-pro",
            "gemini-1.5-pro-latest",
            "models/gemini-2.0-pro-exp",
            "models/gemini-2.0-pro-exp-02-05"
        ]
        return self.model_name in pro_models

config = Config()

AVAILABLE_MODELS = [
    {"full": "gemini-1.5-pro-latest", "short": "1.5-pro-latest"},
    {"full": "gemini-1.0-pro", "short": "1.0-pro"},
    {"full": "gemini-1.5-flash", "short": "1.5-flash"},
    {"full": "models/gemini-2.0-flash-exp", "short": "2.0-flash-exp"},
    {"full": "models/gemini-2.0-flash", "short": "2.0-flash"},
    {"full": "models/gemini-2.0-flash-001", "short": "2.0-flash-001"},
    {"full": "models/gemini-2.0-flash-exp-image-generation", "short": "2.0-flash-exp-img"},
    {"full": "models/gemini-2.0-flash-lite-001", "short": "2.0-flash-lite-001"},
    {"full": "models/gemini-2.0-flash-lite", "short": "2.0-flash-lite"},
    {"full": "models/gemini-2.0-flash-lite-preview-02-05", "short": "2.0-flash-lite-p-0205"},
    {"full": "models/gemini-2.0-flash-lite-preview", "short": "2.0-flash-lite-p"},
    {"full": "models/gemini-2.0-pro-exp", "short": "2.0-pro-exp"},
    {"full": "models/gemini-2.0-pro-exp-02-05", "short": "2.0-pro-exp-0205"},
    {"full": "models/gemini-exp-1206", "short": "exp-1206"},
    {"full": "models/gemini-2.0-flash-thinking-exp-01-21", "short": "2.0-flash-think-0121"},
    {"full": "models/gemini-2.0-flash-thinking-exp", "short": "2.0-flash-think-exp"},
    {"full": "models/gemini-2.0-flash-thinking-exp-1219", "short": "2.0-flash-think-1219"}
]

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s.,!?]', '', text)
    return text.strip()

def split_text(text, chunk_size=config.chunk_size):
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

def generate_questions(chunk, mode):
    if not config.model:
        if not config.api_key:
            return "‚ùå ‡§™‡§π‡§≤‡•á Gemini API ‡§ï‡•Å‡§Ç‡§ú‡•Ä ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç (/setkey <key>)‡•§"
        config.set_model(config.model_name)
    try:
        prompt = config.mcq_prompt if mode == "mcq" else config.simple_prompt
        response = config.model.generate_content(
            prompt.format(chunk),
            generation_config=genai.types.GenerationConfig(max_output_tokens=config.max_tokens)
        )
        return response.text.strip()
    except Exception as e:
        logger.error(f"Error generating questions: {e}")
        return f"‚ùå ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}"

def ocr_image(image_path):
    try:
        ocr_model = genai.GenerativeModel(config.fixed_ocr_model)
        with open(image_path, "rb") as img_file:
            img_data = img_file.read()
        response = ocr_model.generate_content(
            ["Extract text from this image:", {"mime_type": "image/jpeg", "data": img_data}]
        )
        return response.text.strip()
    except Exception as e:
        logger.error(f"OCR error: {e}")
        return f"‚ùå OCR ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}"

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üëã **‡§π‡§æ‡§Ø!** ‡§Æ‡•Å‡§ù‡•á ‡§è‡§ï .txt, .pdf, ‡§Ø‡§æ ‡§õ‡§µ‡§ø (.jpg/.png/‡§´‡•ã‡§ü‡•ã) ‡§≠‡•á‡§ú‡•á‡§Ç, ‡§î‡§∞ ‡§Æ‡•à‡§Ç ‡§â‡§∏‡§∏‡•á ‡§ï‡•ç‡§µ‡§ø‡§ú‡§º ‡§¨‡§®‡§æ ‡§¶‡•Ç‡§Ç‡§ó‡§æ‡•§\n\n"
        "üìå **‡§ï‡•à‡§∏‡•á ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à:**\n"
        "- ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü: MCQ ‡§Æ‡•ã‡§° (`gemini-2.0-pro-exp` ‡§Æ‡•â‡§°‡§≤)\n"
        "- **Simple Mode**: ‡§Æ‡•à‡§®‡•ç‡§Ø‡•Å‡§Ö‡§≤ ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç‡•§\n"
        "- **MCQ Mode**: ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§æ-‡§â‡§®‡•ç‡§Æ‡•Å‡§ñ MCQs ‡§™‡•Ç‡§∞‡•á ‡§ü‡•â‡§™‡§ø‡§ï ‡§ï‡•ã ‡§ï‡§µ‡§∞ ‡§ï‡§∞‡•á‡§Ç‡§ó‡•á‡•§\n"
        "- `/setmode simple` ‡§Ø‡§æ `/setmode mcq` ‡§∏‡•á ‡§Æ‡•ã‡§° ‡§¨‡§¶‡§≤‡•á‡§Ç‡•§\n\n"
        "üîπ **‡§ï‡§Æ‡§æ‡§Ç‡§°‡•ç‡§∏:**\n"
        "- `/start` - ‡§¨‡•â‡§ü ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•á‡§Ç\n"
        "- `/setkey <key>` - Gemini API ‡§ï‡•Å‡§Ç‡§ú‡•Ä ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç\n"
        "- `/listmodels` - ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§Æ‡•â‡§°‡§≤ ‡§¶‡•á‡§ñ‡•á‡§Ç ‡§î‡§∞ ‡§ö‡•Å‡§®‡•á‡§Ç\n"
        "- `/setmode <simple/mcq>` - ‡§ï‡•ç‡§µ‡§ø‡§ú‡§º ‡§Æ‡•ã‡§° ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç\n"
        "- `/setprompt <prompt>` - Simple ‡§Æ‡•ã‡§° ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç\n"
        "- `/help` - ‡§∏‡§≠‡•Ä ‡§ï‡§Æ‡§æ‡§Ç‡§° ‡§¶‡•á‡§ñ‡•á‡§Ç",
        parse_mode="Markdown"
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üìå **‡§∏‡§≠‡•Ä ‡§ï‡§Æ‡§æ‡§Ç‡§°‡•ç‡§∏:**\n\n"
        "- `/start` - ‡§¨‡•â‡§ü ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•á‡§Ç\n"
        "- `/setkey <key>` - Gemini API ‡§ï‡•Å‡§Ç‡§ú‡•Ä ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç\n"
        "- `/listmodels` - ‡§â‡§™‡§≤‡§¨‡•ç‡§ß Gemini ‡§Æ‡•â‡§°‡§≤ ‡§¶‡•á‡§ñ‡•á‡§Ç ‡§î‡§∞ ‡§ö‡•Å‡§®‡•á‡§Ç\n"
        "- `/setmode <simple/mcq>` - ‡§ï‡•ç‡§µ‡§ø‡§ú‡§º ‡§Æ‡•ã‡§° ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç\n"
        "- `/setprompt <prompt>` - Simple ‡§Æ‡•ã‡§° ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§∏‡•ç‡§ü‡§Æ ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç\n"
        "- `/help` - ‡§Ø‡§π ‡§∏‡§Ç‡§¶‡•á‡§∂ ‡§¶‡§ø‡§ñ‡§æ‡§è‡§Å",
        parse_mode="Markdown"
    )

async def set_key(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not context.args:
        await update.message.reply_text("‚ùå ‡§ï‡•É‡§™‡§Ø‡§æ API ‡§ï‡•Å‡§Ç‡§ú‡•Ä ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£: `/setkey your-api-key`", parse_mode="Markdown")
        return
    api_key = " ".join(context.args)
    config.set_api_key(api_key)
    await update.message.reply_text("‚úÖ Gemini API ‡§ï‡•Å‡§Ç‡§ú‡•Ä ‡§∏‡•á‡§ü ‡§π‡•ã ‡§ó‡§à‡•§ ‡§Ö‡§¨ ‡§Æ‡•â‡§°‡§≤ ‡§ö‡•Å‡§®‡•á‡§Ç: `/listmodels` ‡§Ø‡§æ ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§", parse_mode="Markdown")

async def list_models(update: Update, context: ContextTypes.DEFAULT_TYPE):
    keyboard = [[InlineKeyboardButton(model["short"], callback_data=f"model_{model['full']}")] for model in AVAILABLE_MODELS]
    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text("üìå **‡§â‡§™‡§≤‡§¨‡•ç‡§ß Gemini ‡§Æ‡•â‡§°‡§≤:**\n‡§ï‡•É‡§™‡§Ø‡§æ ‡§è‡§ï ‡§Æ‡•â‡§°‡§≤ ‡§ö‡•Å‡§®‡•á‡§Ç:", reply_markup=reply_markup, parse_mode="Markdown")

async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    if query.data.startswith("model_"):
        model_name = query.data[len("model_"):]
        config.set_model(model_name)
        await query.edit_message_text(f"‚úÖ ‡§Æ‡•â‡§°‡§≤ `{model_name}` ‡§∏‡•á‡§ü ‡§π‡•ã ‡§ó‡§Ø‡§æ‡•§ ‡§Ö‡§¨ ‡§´‡§æ‡§á‡§≤ ‡§Ø‡§æ ‡§õ‡§µ‡§ø ‡§≠‡•á‡§ú‡•á‡§Ç‡•§", parse_mode="Markdown")

async def set_mode(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not context.args or context.args[0].lower() not in ["simple", "mcq"]:
        await update.message.reply_text("‚ùå ‡§ï‡•É‡§™‡§Ø‡§æ `simple` ‡§Ø‡§æ `mcq` ‡§ö‡•Å‡§®‡•á‡§Ç‡•§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£: `/setmode mcq`", parse_mode="Markdown")
        return
    mode = context.args[0].lower()
    config.set_mode(mode)
    await update.message.reply_text(f"‚úÖ ‡§Æ‡•ã‡§° `{mode}` ‡§∏‡•á‡§ü ‡§π‡•ã ‡§ó‡§Ø‡§æ‡•§ ‡§Ö‡§¨ ‡§´‡§æ‡§á‡§≤ ‡§Ø‡§æ ‡§õ‡§µ‡§ø ‡§≠‡•á‡§ú‡•á‡§Ç‡•§", parse_mode="Markdown")

async def set_prompt(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not context.args:
        await update.message.reply_text("‚ùå ‡§ï‡•É‡§™‡§Ø‡§æ ‡§è‡§ï ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£: `/setprompt ‡§®‡§Ø‡§æ ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü`", parse_mode="Markdown")
        return
    prompt_text = " ".join(context.args)
    config.simple_prompt = prompt_text + "\nText:\n{}"
    config.set_mode("simple")
    await update.message.reply_text("‚úÖ Simple ‡§Æ‡•ã‡§° ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§∏‡•ç‡§ü‡§Æ ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§∏‡•á‡§ü ‡§π‡•ã ‡§ó‡§Ø‡§æ‡•§ ‡§Ö‡§¨ ‡§´‡§æ‡§á‡§≤ ‡§Ø‡§æ ‡§õ‡§µ‡§ø ‡§≠‡•á‡§ú‡•á‡§Ç‡•§", parse_mode="Markdown")

def parse_mcq(text):
    mcqs = []
    pattern = r"Q: (.+?)\nA: (.+?)\nB: (.+?)\nC: (.+?)\nD: (.+?)\nCorrect: ([ABCD])\nE: (.+?)(?=\nQ:|$)"
    matches = re.finditer(pattern, text, re.DOTALL)
    for match in matches:
        question = match.group(1).strip()[:256]
        options = [match.group(i).strip()[:100] for i in range(2, 6)]
        correct = {"A": 0, "B": 1, "C": 2, "D": 3}[match.group(6).strip()]
        explanation = match.group(7).strip()[:1024]
        mcqs.append({"question": question, "options": options, "correct": correct, "explanation": explanation})
    return mcqs

def parse_simple(text):
    simples = []
    pattern = r"\*\*Q:\*\* (.+?) \| \*\*A:\*\* (.+?) \| \*\*E:\*\* (.+?)(?=\n\*\*Q:\*\*|$)"
    matches = re.finditer(pattern, text, re.DOTALL)
    for match in matches:
        question = match.group(1).strip()
        answer = match.group(2).strip()
        explanation = match.group(3).strip()
        simples.append({"question": question, "answer": answer, "explanation": explanation})
    return simples

async def process_text_and_generate_quiz(update: Update, text_content, progress_msg):
    try:
        text_content = clean_text(text_content)
        if not text_content:
            await progress_msg.edit_text("‚ùå ‡§ï‡•ã‡§à ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§®‡§ø‡§ï‡§æ‡§≤‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§æ‡•§", parse_mode="Markdown")
            return

        chunks = split_text(text_content)
        total_chunks = len(chunks)
        quiz_output = ""
        is_pro_model = config.is_pro_model() and config.mode == "mcq"

        for i, chunk in enumerate(chunks):
            await progress_msg.edit_text(f"üîÑ **‡§™‡•ç‡§∞‡§ó‡§§‡§ø:** ‡§ö‡§Ç‡§ï {i+1}/{total_chunks} ‡§∏‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§¨‡§® ‡§∞‡§π‡•á ‡§π‡•à‡§Ç...", parse_mode="Markdown")
            quiz_output += generate_questions(chunk, config.mode) + "\n\n"
            if is_pro_model and i < total_chunks - 1:
                await progress_msg.edit_text(f"‚è≥ **‡§™‡•ç‡§∞‡§§‡•Ä‡§ï‡•ç‡§∑‡§æ:** ‡§ö‡§Ç‡§ï {i+1}/{total_chunks} ‡§™‡•Ç‡§∞‡§æ ‡§π‡•Å‡§Ü‡•§ 30 ‡§∏‡•á‡§ï‡§Ç‡§° ‡§ï‡§æ ‡§Ö‡§Ç‡§§‡§∞‡§æ‡§≤...", parse_mode="Markdown")
                time.sleep(30)

        if config.mode == "mcq":
            mcqs = parse_mcq(quiz_output)
            if not mcqs:
                await progress_msg.edit_text("‚ùå MCQ ‡§™‡§æ‡§∞‡•ç‡§∏ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø‡•§ ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§ö‡•á‡§ï ‡§ï‡§∞‡•á‡§Ç‡•§", parse_mode="Markdown")
                return
            await progress_msg.edit_text(f"‚úÖ **MCQ ‡§ï‡•ç‡§µ‡§ø‡§ú‡§º ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à!** ({len(mcqs)} ‡§∏‡§µ‡§æ‡§≤)", parse_mode="Markdown")
            for i, mcq in enumerate(mcqs, 1):
                logger.info(f"Poll {i}: Q: {mcq['question']} | Options: {mcq['options']} | Correct: {mcq['correct']} | E: {mcq['explanation']}")
                logger.info(f"Lengths - Q: {len(mcq['question'])}, Options: {[len(opt) for opt in mcq['options']]}, E: {len(mcq['explanation'])}")

                question = mcq['question'][:256]
                options = [opt[:100] for opt in mcq['options']][:10]
                explanation = mcq['explanation'][:1024]

                if len(question) > 256 or any(len(opt) > 100 for opt in options) or len(explanation) > 1024:
                    logger.error(f"Invalid lengths in question {i}")
                    continue

                try:
                    await update.message.reply_poll(
                        question=f"‡§™‡•ç‡§∞‡§∂‡•ç‡§® {i}: {question}",
                        options=options,
                        type="quiz",
                        correct_option_id=mcq['correct'],
                        is_anonymous=False,
                        explanation=explanation,
                        explanation_parse_mode="Markdown"
                    )
                except Exception as e:
                    logger.error(f"Poll {i} error: {e}")
                    # Fallback to text output for any poll error
                    text_output = (
                        f"**‡§™‡•ç‡§∞‡§∂‡•ç‡§® {i}:** {mcq['question']}\n"
                        f"**A:** {mcq['options'][0]}\n"
                        f"**B:** {mcq['options'][1]}\n"
                        f"**C:** {mcq['options'][2]}\n"
                        f"**D:** {mcq['options'][3]}\n"
                        f"**‡§∏‡§π‡•Ä ‡§â‡§§‡•ç‡§§‡§∞:** {mcq['options'][mcq['correct']]}\n"
                        f"**‡§µ‡§ø‡§µ‡§∞‡§£:** {mcq['explanation']}\n"
                    )
                    if len(text_output) > 4000:
                        parts = [text_output[i:i + 4000] for i in range(0, len(text_output), 4000)]
                        for part in parts:
                            await update.message.reply_text(part, parse_mode="Markdown")
                    else:
                        await update.message.reply_text(text_output, parse_mode="Markdown")
                    await update.message.reply_text(
                        f"‚ö†Ô∏è ‡§™‡•ç‡§∞‡§∂‡•ç‡§® {i} ‡§ï‡•ã ‡§™‡•ã‡§≤ ‡§ï‡•á ‡§¨‡§ú‡§æ‡§Ø ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§≠‡•á‡§ú‡§æ ‡§ó‡§Ø‡§æ ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§π‡•Å‡§à: {e}",
                        parse_mode="Markdown"
                    )
        else:  # Simple mode
            simples = parse_simple(quiz_output)
            if not simples:
                await progress_msg.edit_text("‚ùå Simple ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡§æ‡§∞‡•ç‡§∏ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø‡•§ ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§ö‡•á‡§ï ‡§ï‡§∞‡•á‡§Ç‡•§", parse_mode="Markdown")
                return
            output_parts = []
            current_part = f"‚úÖ **Simple ‡§ï‡•ç‡§µ‡§ø‡§ú‡§º ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à!** ({len(simples)} ‡§∏‡§µ‡§æ‡§≤)\n\n"
            
            for i, simple in enumerate(simples, 1):
                question_block = (
                    f"**‡§™‡•ç‡§∞‡§∂‡•ç‡§® {i}:** {simple['question']}\n"
                    f"**‡§â‡§§‡•ç‡§§‡§∞:** {simple['answer']}\n"
                    f"**‡§µ‡§ø‡§µ‡§∞‡§£:** {simple['explanation']}\n\n"
                )
                if len(current_part) + len(question_block) > 4000:
                    output_parts.append(current_part)
                    current_part = question_block
                else:
                    current_part += question_block
            
            if current_part:
                output_parts.append(current_part)
            
            for part in output_parts:
                await update.message.reply_text(part, parse_mode="Markdown")
            await progress_msg.delete()
    except Exception as e:
        logger.error(f"Quiz generation error: {e}")
        await progress_msg.edit_text(f"‚ùå ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}", parse_mode="Markdown")

async def handle_file(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not config.api_key:
        await update.message.reply_text("‚ùå ‡§™‡§π‡§≤‡•á `/setkey <key>` ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç‡•§", parse_mode="Markdown")
        return

    progress_msg = await update.message.reply_text("‚è≥ **‡§™‡•ç‡§∞‡§ó‡§§‡§ø:** ‡§´‡§æ‡§á‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à...", parse_mode="Markdown")
    text_content = ""

    try:
        if update.message.document:
            file = update.message.document
            file_obj = await context.bot.get_file(file.file_id)
            file_path = await file_obj.download_to_drive()

            if file.mime_type == "text/plain":
                await progress_msg.edit_text("üìù **‡§™‡•ç‡§∞‡§ó‡§§‡§ø:** .txt ‡§´‡§æ‡§á‡§≤ ‡§™‡§¢‡§º‡•Ä ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à...", parse_mode="Markdown")
                with open(file_path, 'r', encoding='utf-8') as f:
                    text_content = f.read()
            elif file.mime_type == "application/pdf":
                await progress_msg.edit_text("üìÑ **‡§™‡•ç‡§∞‡§ó‡§§‡§ø:** .pdf ‡§´‡§æ‡§á‡§≤ ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à...", parse_mode="Markdown")
                images = convert_from_path(file_path)
                total_pages = len(images)
                for i, image in enumerate(images):
                    image_path = f"temp_page_{i}.jpg"
                    image.save(image_path, "JPEG")
                    await progress_msg.edit_text(f"üñºÔ∏è **‡§™‡•ç‡§∞‡§ó‡§§‡§ø:** ‡§™‡•á‡§ú {i+1}/{total_pages} ‡§ï‡§æ OCR ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à...", parse_mode="Markdown")
                    text_content += ocr_image(image_path) + "\n"
                    os.remove(image_path)
            elif file.mime_type in ["image/jpeg", "image/png"]:
                await progress_msg.edit_text("üñºÔ∏è **‡§™‡•ç‡§∞‡§ó‡§§‡§ø:** ‡§õ‡§µ‡§ø ‡§ï‡§æ OCR ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à...", parse_mode="Markdown")
                text_content = ocr_image(file_path)
            else:
                await progress_msg.edit_text("‚ùå ‡§ï‡•É‡§™‡§Ø‡§æ .txt, .pdf, ‡§Ø‡§æ .jpg/.png ‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ú‡•á‡§Ç‡•§", parse_mode="Markdown")
                os.remove(file_path)
                return

            os.remove(file_path)
            await process_text_and_generate_quiz(update, text_content, progress_msg)

    except Exception as e:
        logger.error(f"File processing error: {e}")
        await progress_msg.edit_text(f"‚ùå ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}", parse_mode="Markdown")

async def handle_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not config.api_key:
        await update.message.reply_text("‚ùå ‡§™‡§π‡§≤‡•á `/setkey <key>` ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç‡•§", parse_mode="Markdown")
        return

    progress_msg = await update.message.reply_text("‚è≥ **‡§™‡•ç‡§∞‡§ó‡§§‡§ø:** ‡§´‡•ã‡§ü‡•ã ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à...", parse_mode="Markdown")
    photo = update.message.photo[-1]
    file_obj = await photo.get_file()
    file_path = await file_obj.download_to_drive()

    try:
        await progress_msg.edit_text("üñºÔ∏è **‡§™‡•ç‡§∞‡§ó‡§§‡§ø:** ‡§´‡•ã‡§ü‡•ã ‡§ï‡§æ OCR ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à...", parse_mode="Markdown")
        text_content = ocr_image(file_path)
        await process_text_and_generate_quiz(update, text_content, progress_msg)
    except Exception as e:
        logger.error(f"Photo processing error: {e}")
        await progress_msg.edit_text(f"‚ùå ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}", parse_mode="Markdown")
    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

async def error(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.error(f"Bot error: {context.error}")
    await update.message.reply_text(f"‚ùå ‡§ï‡•Å‡§õ ‡§ó‡§≤‡§§ ‡§π‡•ã ‡§ó‡§Ø‡§æ: {context.error}", parse_mode="Markdown")

def main():
    app = Application.builder().token("7779216065:AAEeAkWfwP0-HyTi2CQeJ6iULvGkW-EqamQ").build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(CommandHandler("setkey", set_key))
    app.add_handler(CommandHandler("listmodels", list_models))
    app.add_handler(CommandHandler("setmode", set_mode))
    app.add_handler(CommandHandler("setprompt", set_prompt))
    app.add_handler(MessageHandler(filters.Document.ALL, handle_file))
    app.add_handler(MessageHandler(filters.PHOTO, handle_photo))
    app.add_handler(CallbackQueryHandler(button_handler, pattern="^model_"))
    app.add_error_handler(error)
    app.run_polling()

if __name__ == "__main__":
    main()